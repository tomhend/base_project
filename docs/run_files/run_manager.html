<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>base_project.run_files.run_manager API documentation</title>
<meta name="description" content="File containing the RunManager class which handles the initaliazation of all components necessary
for a run and progresses the run." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>base_project.run_files.run_manager</code></h1>
</header>
<section id="section-intro">
<p>File containing the RunManager class which handles the initaliazation of all components necessary
for a run and progresses the run.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
File containing the RunManager class which handles the initaliazation of all components necessary
for a run and progresses the run.
&#34;&#34;&#34;

import logging
from typing import Tuple

import numpy as np
import torch
from torch.utils.data import DataLoader
import wandb

from run_files.metrics import Metrics
from run_files.run_logger import RunLogger
from builders import (
    dataset_builder,
    loss_builder,
    model_builder,
    optimizer_builder,
    trainer_builder,
)


class RunManager:
    &#34;&#34;&#34;
    This class initalizes all components necessary to execute the run, this is done following the
    configuration dictionary passed as an argument to the init function. The config file options and
    necessities are defined in a seperate file TODO: add file name.

    Attributes:
        cfg (dict[str, any]): configuration dictionary of the run
        logger (RunLogger, optional): logger that is used
        train_dataloader (DataLoader): dataloader containing the training data
        val_dataloader (DataLoader): dataloader containing the validation data
        model (nn.Module): the pytorch model that will be trained
        optimizer (torch.optim.Optimizer): optimizer that will be used
        loss_fn (torch.nn.modules.loss._Loss): loss function that will be used
        device (str): name of the device to train on
        metrics (Metrics): instance of Metrics that handles the metric calculation
        selection_metric (str): name of the metric to select the best model on
        goal (str): string defining if the selection metric should be maximized or minimized
        trainer (trainers.base_trainer.BaseTrainer): trainer handeling the training of the model
        epochs (int): the number of epochs to run
    &#34;&#34;&#34;

    def __init__(self, cfg: dict[str, any]) -&gt; None:
        &#34;&#34;&#34;
        Initializes the RunManager given the cfg parameter.

        Args:
            cfg (dict[str, any]): the configuration dictionary for the run
        &#34;&#34;&#34;

        self.cfg = cfg
        self.epochs = self.cfg[&#34;session_cfg&#34;][&#34;epochs&#34;]
        self.logger = None

        # If log_cfg exists, build a logger
        if &#34;log_cfg&#34; in cfg:
            self.logger = RunLogger(cfg)
            # When doing wandb sweeps, the configuration is changed and stored in wandb.config
            # so it should be used in runs instead of cfg
            self.cfg = wandb.config

        self.train_dataloader, self.val_dataloader = self._create_dataloaders(self.cfg)

        model_cfg = self.cfg[&#34;model_cfg&#34;]
        self.model = model_builder.build_model(model_cfg[&#34;name&#34;], **model_cfg[&#34;kwargs&#34;])

        optimizer_cfg = self.cfg[&#34;optimizer_cfg&#34;]
        self.optimizer = optimizer_builder.build_optimizer(
            optimizer_cfg[&#34;name&#34;], self.model.parameters(), **optimizer_cfg[&#34;kwargs&#34;]
        )

        loss_cfg = self.cfg[&#34;loss_cfg&#34;]
        self.loss_fn = loss_builder.build_loss_function(
            loss_cfg[&#34;name&#34;], **loss_cfg.get(&#34;kwargs&#34;, {})
        )

        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
        logging.info(&#34;Running on %s&#34;, self.device)

        # Ensure that loss_train_epoch and loss_val_epoch are in the metric list as these are
        # required for training
        metric_list = list(
            set(
                [&#34;loss_train_epoch&#34;, &#34;loss_val_epoch&#34;]
                + self.cfg[&#34;session_cfg&#34;].get(&#34;metrics&#34;, [])
            )
        )
        self.metrics = Metrics(metric_list)
        self.selection_metric = self.cfg[&#34;session_cfg&#34;].get(
            &#34;selection_metric&#34;, &#34;loss_val_epoch&#34;
        )
        self.goal = self.cfg[&#34;session_cfg&#34;].get(&#34;goal&#34;, &#34;minimize&#34;)

        trainer_cfg = self.cfg[&#34;trainer_cfg&#34;]
        self.trainer = trainer_builder.build_trainer(
            trainer_cfg[&#34;name&#34;],
            self.model,
            self.loss_fn,
            self.optimizer,
            self.device,
            self.metrics,
            **trainer_cfg.get(&#34;kwargs&#34;, {}),
        )
        if self.logger:
            self.trainer.set_run_logger(self.logger)

    def start_training(self) -&gt; None:
        &#34;&#34;&#34;
        Starts the training and keeps running for the number of epochs in self.epochs, this method
        also handles the logic for saving the best model. NOTE: possibly seperate model saving into
        another method
        &#34;&#34;&#34;
        best_metric_value = np.Inf
        if self.goal == &#34;maximize&#34;:
            best_metric_value = -np.Inf

        for i in range(self.epochs):
            logging.info(&#34;starting training epoch %s&#34;, i)
            metrics_dict = self.trainer.train_epoch(self.train_dataloader, i)
            logging.info(&#34;starting validation epoch %s&#34;, i)
            validation_metrics_dict = self.trainer.val_epoch(self.val_dataloader, i)

            metrics_dict.update(validation_metrics_dict)
            model_name = (
                wandb.run.name
                if self.logger
                else f&#39;{self.cfg[&#34;model_cfg&#34;][&#34;name&#34;]}_best&#39;
            )

            if self.goal == &#34;maximize&#34;:
                if metrics_dict[self.selection_metric] &gt; best_metric_value:
                    logging.info(
                        &#34;Best value for %s, %s&#34;,
                        self.selection_metric,
                        metrics_dict[self.selection_metric],
                    )
                    torch.save(
                        self.model.state_dict(),
                        f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
                    )
                continue

            if metrics_dict[self.selection_metric] &lt; best_metric_value:
                logging.info(
                    &#34;Best value for %s, %s&#34;,
                    self.selection_metric,
                    metrics_dict[self.selection_metric],
                )
                torch.save(
                    self.model.state_dict(),
                    f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
                )

    @staticmethod
    def _create_dataloaders(cfg: dict) -&gt; Tuple[DataLoader, DataLoader]:
        &#34;&#34;&#34;
        Helper method that creates the dataloaders (as this is quite verbose)

        Args:
            cfg (dict): configuration file containing the train and validation set configurations

        Returns:
            Tuple[DataLoader, DataLoader]: the train and validation dataloaders
        &#34;&#34;&#34;
        train_dataset_cfg = cfg[&#34;train_dataset_cfg&#34;]
        train_dataset = dataset_builder.build_dataset(
            train_dataset_cfg[&#34;name&#34;], **train_dataset_cfg[&#34;kwargs&#34;]
        )

        val_dataset_cfg = cfg[&#34;val_dataset_cfg&#34;]
        val_dataset = dataset_builder.build_dataset(
            val_dataset_cfg[&#34;name&#34;], **val_dataset_cfg[&#34;kwargs&#34;]
        )

        train_dataloader_cfg = cfg[&#34;train_dataloader_cfg&#34;]
        train_dataloader = DataLoader(train_dataset, **train_dataloader_cfg)

        val_dataloader_cfg = cfg[&#34;val_dataloader_cfg&#34;]
        val_dataloader = DataLoader(val_dataset, **val_dataloader_cfg)

        return train_dataloader, val_dataloader</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="base_project.run_files.run_manager.RunManager"><code class="flex name class">
<span>class <span class="ident">RunManager</span></span>
<span>(</span><span>cfg: dict[str, any])</span>
</code></dt>
<dd>
<div class="desc"><p>This class initalizes all components necessary to execute the run, this is done following the
configuration dictionary passed as an argument to the init function. The config file options and
necessities are defined in a seperate file TODO: add file name.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>cfg</code></strong> :&ensp;<code>dict[str, any]</code></dt>
<dd>configuration dictionary of the run</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>RunLogger</code>, optional</dt>
<dd>logger that is used</dd>
<dt><strong><code>train_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>dataloader containing the training data</dd>
<dt><strong><code>val_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>dataloader containing the validation data</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>nn.Module</code></dt>
<dd>the pytorch model that will be trained</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>optimizer that will be used</dd>
<dt><strong><code>loss_fn</code></strong> :&ensp;<code>torch.nn.modules.loss._Loss</code></dt>
<dd>loss function that will be used</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the device to train on</dd>
<dt><strong><code>metrics</code></strong> :&ensp;<code>Metrics</code></dt>
<dd>instance of Metrics that handles the metric calculation</dd>
<dt><strong><code>selection_metric</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the metric to select the best model on</dd>
<dt><strong><code>goal</code></strong> :&ensp;<code>str</code></dt>
<dd>string defining if the selection metric should be maximized or minimized</dd>
<dt><strong><code>trainer</code></strong> :&ensp;<code>trainers.base_trainer.BaseTrainer</code></dt>
<dd>trainer handeling the training of the model</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>the number of epochs to run</dd>
</dl>
<p>Initializes the RunManager given the cfg parameter.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cfg</code></strong> :&ensp;<code>dict[str, any]</code></dt>
<dd>the configuration dictionary for the run</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RunManager:
    &#34;&#34;&#34;
    This class initalizes all components necessary to execute the run, this is done following the
    configuration dictionary passed as an argument to the init function. The config file options and
    necessities are defined in a seperate file TODO: add file name.

    Attributes:
        cfg (dict[str, any]): configuration dictionary of the run
        logger (RunLogger, optional): logger that is used
        train_dataloader (DataLoader): dataloader containing the training data
        val_dataloader (DataLoader): dataloader containing the validation data
        model (nn.Module): the pytorch model that will be trained
        optimizer (torch.optim.Optimizer): optimizer that will be used
        loss_fn (torch.nn.modules.loss._Loss): loss function that will be used
        device (str): name of the device to train on
        metrics (Metrics): instance of Metrics that handles the metric calculation
        selection_metric (str): name of the metric to select the best model on
        goal (str): string defining if the selection metric should be maximized or minimized
        trainer (trainers.base_trainer.BaseTrainer): trainer handeling the training of the model
        epochs (int): the number of epochs to run
    &#34;&#34;&#34;

    def __init__(self, cfg: dict[str, any]) -&gt; None:
        &#34;&#34;&#34;
        Initializes the RunManager given the cfg parameter.

        Args:
            cfg (dict[str, any]): the configuration dictionary for the run
        &#34;&#34;&#34;

        self.cfg = cfg
        self.epochs = self.cfg[&#34;session_cfg&#34;][&#34;epochs&#34;]
        self.logger = None

        # If log_cfg exists, build a logger
        if &#34;log_cfg&#34; in cfg:
            self.logger = RunLogger(cfg)
            # When doing wandb sweeps, the configuration is changed and stored in wandb.config
            # so it should be used in runs instead of cfg
            self.cfg = wandb.config

        self.train_dataloader, self.val_dataloader = self._create_dataloaders(self.cfg)

        model_cfg = self.cfg[&#34;model_cfg&#34;]
        self.model = model_builder.build_model(model_cfg[&#34;name&#34;], **model_cfg[&#34;kwargs&#34;])

        optimizer_cfg = self.cfg[&#34;optimizer_cfg&#34;]
        self.optimizer = optimizer_builder.build_optimizer(
            optimizer_cfg[&#34;name&#34;], self.model.parameters(), **optimizer_cfg[&#34;kwargs&#34;]
        )

        loss_cfg = self.cfg[&#34;loss_cfg&#34;]
        self.loss_fn = loss_builder.build_loss_function(
            loss_cfg[&#34;name&#34;], **loss_cfg.get(&#34;kwargs&#34;, {})
        )

        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
        logging.info(&#34;Running on %s&#34;, self.device)

        # Ensure that loss_train_epoch and loss_val_epoch are in the metric list as these are
        # required for training
        metric_list = list(
            set(
                [&#34;loss_train_epoch&#34;, &#34;loss_val_epoch&#34;]
                + self.cfg[&#34;session_cfg&#34;].get(&#34;metrics&#34;, [])
            )
        )
        self.metrics = Metrics(metric_list)
        self.selection_metric = self.cfg[&#34;session_cfg&#34;].get(
            &#34;selection_metric&#34;, &#34;loss_val_epoch&#34;
        )
        self.goal = self.cfg[&#34;session_cfg&#34;].get(&#34;goal&#34;, &#34;minimize&#34;)

        trainer_cfg = self.cfg[&#34;trainer_cfg&#34;]
        self.trainer = trainer_builder.build_trainer(
            trainer_cfg[&#34;name&#34;],
            self.model,
            self.loss_fn,
            self.optimizer,
            self.device,
            self.metrics,
            **trainer_cfg.get(&#34;kwargs&#34;, {}),
        )
        if self.logger:
            self.trainer.set_run_logger(self.logger)

    def start_training(self) -&gt; None:
        &#34;&#34;&#34;
        Starts the training and keeps running for the number of epochs in self.epochs, this method
        also handles the logic for saving the best model. NOTE: possibly seperate model saving into
        another method
        &#34;&#34;&#34;
        best_metric_value = np.Inf
        if self.goal == &#34;maximize&#34;:
            best_metric_value = -np.Inf

        for i in range(self.epochs):
            logging.info(&#34;starting training epoch %s&#34;, i)
            metrics_dict = self.trainer.train_epoch(self.train_dataloader, i)
            logging.info(&#34;starting validation epoch %s&#34;, i)
            validation_metrics_dict = self.trainer.val_epoch(self.val_dataloader, i)

            metrics_dict.update(validation_metrics_dict)
            model_name = (
                wandb.run.name
                if self.logger
                else f&#39;{self.cfg[&#34;model_cfg&#34;][&#34;name&#34;]}_best&#39;
            )

            if self.goal == &#34;maximize&#34;:
                if metrics_dict[self.selection_metric] &gt; best_metric_value:
                    logging.info(
                        &#34;Best value for %s, %s&#34;,
                        self.selection_metric,
                        metrics_dict[self.selection_metric],
                    )
                    torch.save(
                        self.model.state_dict(),
                        f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
                    )
                continue

            if metrics_dict[self.selection_metric] &lt; best_metric_value:
                logging.info(
                    &#34;Best value for %s, %s&#34;,
                    self.selection_metric,
                    metrics_dict[self.selection_metric],
                )
                torch.save(
                    self.model.state_dict(),
                    f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
                )

    @staticmethod
    def _create_dataloaders(cfg: dict) -&gt; Tuple[DataLoader, DataLoader]:
        &#34;&#34;&#34;
        Helper method that creates the dataloaders (as this is quite verbose)

        Args:
            cfg (dict): configuration file containing the train and validation set configurations

        Returns:
            Tuple[DataLoader, DataLoader]: the train and validation dataloaders
        &#34;&#34;&#34;
        train_dataset_cfg = cfg[&#34;train_dataset_cfg&#34;]
        train_dataset = dataset_builder.build_dataset(
            train_dataset_cfg[&#34;name&#34;], **train_dataset_cfg[&#34;kwargs&#34;]
        )

        val_dataset_cfg = cfg[&#34;val_dataset_cfg&#34;]
        val_dataset = dataset_builder.build_dataset(
            val_dataset_cfg[&#34;name&#34;], **val_dataset_cfg[&#34;kwargs&#34;]
        )

        train_dataloader_cfg = cfg[&#34;train_dataloader_cfg&#34;]
        train_dataloader = DataLoader(train_dataset, **train_dataloader_cfg)

        val_dataloader_cfg = cfg[&#34;val_dataloader_cfg&#34;]
        val_dataloader = DataLoader(val_dataset, **val_dataloader_cfg)

        return train_dataloader, val_dataloader</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="base_project.run_files.run_manager.RunManager.start_training"><code class="name flex">
<span>def <span class="ident">start_training</span></span>(<span>self) -> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Starts the training and keeps running for the number of epochs in self.epochs, this method
also handles the logic for saving the best model. NOTE: possibly seperate model saving into
another method</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_training(self) -&gt; None:
    &#34;&#34;&#34;
    Starts the training and keeps running for the number of epochs in self.epochs, this method
    also handles the logic for saving the best model. NOTE: possibly seperate model saving into
    another method
    &#34;&#34;&#34;
    best_metric_value = np.Inf
    if self.goal == &#34;maximize&#34;:
        best_metric_value = -np.Inf

    for i in range(self.epochs):
        logging.info(&#34;starting training epoch %s&#34;, i)
        metrics_dict = self.trainer.train_epoch(self.train_dataloader, i)
        logging.info(&#34;starting validation epoch %s&#34;, i)
        validation_metrics_dict = self.trainer.val_epoch(self.val_dataloader, i)

        metrics_dict.update(validation_metrics_dict)
        model_name = (
            wandb.run.name
            if self.logger
            else f&#39;{self.cfg[&#34;model_cfg&#34;][&#34;name&#34;]}_best&#39;
        )

        if self.goal == &#34;maximize&#34;:
            if metrics_dict[self.selection_metric] &gt; best_metric_value:
                logging.info(
                    &#34;Best value for %s, %s&#34;,
                    self.selection_metric,
                    metrics_dict[self.selection_metric],
                )
                torch.save(
                    self.model.state_dict(),
                    f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
                )
            continue

        if metrics_dict[self.selection_metric] &lt; best_metric_value:
            logging.info(
                &#34;Best value for %s, %s&#34;,
                self.selection_metric,
                metrics_dict[self.selection_metric],
            )
            torch.save(
                self.model.state_dict(),
                f&#34;models/saved_models/trained_models/{model_name}.pt&#34;,
            )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="base_project.run_files" href="index.html">base_project.run_files</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="base_project.run_files.run_manager.RunManager" href="#base_project.run_files.run_manager.RunManager">RunManager</a></code></h4>
<ul class="">
<li><code><a title="base_project.run_files.run_manager.RunManager.start_training" href="#base_project.run_files.run_manager.RunManager.start_training">start_training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>